# Module 20 Individual Self-Assessment

## Part 1 - Self Assessment including challenges faced

The biggest challenge I faced was also the team challenge because I experienced some health issues that just happened to hit during the first week of the project and carried over into the second.  From a technical point of view, all of my ideas during the second week were too grandiose for a single person to complete in under 3 weeks. The topic I decided on is an issue that is actually something I have been interested in and concerned about for some time, even before this class.

Even though I was unable to complete some parts of the project, I still learned a lot, and I may continue to work on this subject as a hobby even after the class ends. I still have a goal of training a neural network with a coda based on my own writings, and find out what insights can be derived. Then I will write it up as a research paper rather than a slide presentation and publish it on my Blog (see below).


## Part 2 - Team Assessment

Due to some health issues that happened to coincide with the first week of the project, I decided in conjunction with the instructor and Student Success Manager, to work individually on my own project.  This meant having two topic changes before finally deciding on a project of my own, and less time to complete all the segments. During the presentations on the last night of class I was impressed with all the teams' presentations, including my former team.  Team member Michael acknowledged that one of the challenges they faced was not having a better plan at the start of the project, not only because of the topic change in the first week.  I understood what he meant, and yet they were able to produce a website and slide presentation that came across as very professional even while acknowledging the limitations of some of the data sources they selected.


## Part 3 - Summary of project 

Natural Language Processing (NLP) Word Embedding Model using  a Deep Learning Neural Network to train a large word vector file.  Used a Global Vector file based on the Wikipedia coda in the public domain and trained the model to show semantic relationships and similarities between words.  Proposal for future work - to create my own large vector file based on a coda of my own writing, aka my Personal Blog (<a href="https://gcmastra.wordpress.com/" target="_blank">https://gcmastra.wordpress.com/</a>)

The statement above can be used as a "blurb" on LinkedIn or on a resume.  The underlying question is "How do autocorrect and next word suggestion algorithms really work?" with the corollary "why is it so common for spell checkers and autocorrect to suggest the WRONG word much of the time?"  So I wanted to figure out how they worked, and see if I could train one using examples of my own writing.  I was able to prove, at least in theory, that it is possible to train a model using a neural network with my own data and produce a word vector file on a small scale.  Then I loaded a pre-trained dataset into an established word embedding algorithm, and explored how it worked when determining word similarity and word analogy.  Finally, I proposed to create my own large word vector file using a coda of my own writing, and then feed it into the second model to see if it does better at predicting the associated words.  As indicated above, even though I didn't complete the third model, I might still work on this on my own as a hobby after class ends. 

